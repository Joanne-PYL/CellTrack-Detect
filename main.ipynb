{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Detectron2\n",
    "Requirement for running Detectron2 include:\n",
    "* gcc & g++ ≥ 5\n",
    "* Python ≥ 3.6\n",
    "* PyTorch ≥ 1.4\n",
    "* torchvision that matches the PyTorch installation\n",
    "* OpenCV\n",
    "* pycocotools: pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n",
    "* fvcore: conda install -c fvcore fvcore\n",
    "* Detectron2:  pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/index.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import itertools\n",
    "import random\n",
    "import glob\n",
    "import torch\n",
    "import time\n",
    "import datetime\n",
    "import logging\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from skimage import data\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.segmentation import clear_border\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage.measure import regionprops_table\n",
    "from skimage.morphology import closing, square\n",
    "from skimage.color import label2rgb, rgb2gray\n",
    "from skimage.measure import find_contours, approximate_polygon\n",
    "\n",
    "import detectron2\n",
    "import detectron2.utils.comm as comm\n",
    "from detectron2.utils.logger import setup_logger, log_every_n_seconds\n",
    "setup_logger()\n",
    "\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.engine.hooks import HookBase\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog, DatasetMapper, build_detection_test_loader\n",
    "from detectron2.utils.visualizer import ColorMode\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.structures import BoxMode\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and preprocessing data \n",
    "* RGB_30frames_D5_patch00: raw data\n",
    "* ANNO_D5_patch00: cell nuclei annotation\n",
    "\n",
    "We put both Nifti and PNG files in Google drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set train:validation:test = 18:6:6 frames\n",
    "data_path = 'data_nifti/'\n",
    "RGB_img = nib.load(data_path + 'RGB_30frames_D5_patch00.nii').get_data()\n",
    "ANNO_img = nib.load(data_path + 'ANNO_D5_patch00.nii').get_fdata()\n",
    "data_num = RGB_img.shape[2]\n",
    "\n",
    "os.makedirs('data_PNG/train', exist_ok=True)\n",
    "os.makedirs('data_PNG/val', exist_ok=True)\n",
    "os.makedirs('data/PNG/test', exist_ok=True)\n",
    "\n",
    "train_path = 'data/PNG/train/'\n",
    "val_path = 'data/PNG/val/'\n",
    "test_path = 'data/PNG/test/'\n",
    "\n",
    "# Save raw data as PNG files \n",
    "for i in range(data_num):\n",
    "    RGB_image = RGB_img[:,:,i]\n",
    "    RGB_frames = RGB_image.view((np.uint8, len(RGB_image.dtype.names)))\n",
    "    if i < 18:\n",
    "        plt.imsave(train_path + 'RGB_frame_' + str(i) + '.png', RGB_frames, format = 'png')\n",
    "    elif i < 24:\n",
    "        plt.imsave(val_path + 'RGB_frame_' + str(i) + '.png', RGB_frames, format = 'png')\n",
    "    else: \n",
    "        plt.imsave(test_path + 'RGB_frame_' + str(i) + '.png', RGB_frames, format = 'png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find contours of each neuclei from annotation files and save them in ANNO_train and ANNO_val. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANNO_train = {}\n",
    "ANNO_val = {}\n",
    "\n",
    "for i in range(data_num): \n",
    "    ANNO_image = ANNO_img[:,:,i]\n",
    "    gray_img = rgb2gray(ANNO_image)\n",
    "    contours = find_contours(gray_img, 0.8)\n",
    "    frame = 'RGB_frame_' + str(i)\n",
    "    cell_per_frame={}\n",
    "  \n",
    "    for n, contour in enumerate(contours):\n",
    "        cell = 'cell_' + str(n)\n",
    "        tmp = {'x' : contour[:,1],'y': contour[:, 0]}\n",
    "        cell_per_frame.update({cell: tmp})\n",
    "\n",
    "    if i < 18:\n",
    "        ANNO_train.update({frame: cell_per_frame})\n",
    "    elif i < 24:  \n",
    "        ANNO_val.update({frame: cell_per_frame})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register the training and validation dataset to detectron2. Write a function to parse it and prepare it into detectron2's standard format.\n",
    "\n",
    "Information for each nuceli must include file_name (path of image), image ID, height and width of image size, bbox (bounding box of nuclei) and segmentation (contours of nuclei)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_dicts(img_dir):\n",
    "    dataset_dicts = []\n",
    "    for i in range(18): \n",
    "        key = 'RGB_frame_' + str(i)       \n",
    "        record = {}       \n",
    "        filename = (train_path + key + '.png')\n",
    "        height, width = ANNO_image.shape[:2]\n",
    "           \n",
    "        record['file_name'] = filename\n",
    "        record['image_id'] = i\n",
    "        record['height'] = height\n",
    "        record['width'] = width\n",
    "\n",
    "        objs = []\n",
    "        for idx, (cell, v) in enumerate(ANNO_train[key].items()):\n",
    "            px = v['x']\n",
    "            py = v['y']            \n",
    "            poly = [(x  , y  )for x, y in zip(px, py)]\n",
    "            poly = [p for x in poly for p in x]\n",
    "            obj = {               \n",
    "                  'bbox': [np.min(px), np.min(py), np.max(px), np.max(py)],\n",
    "                  'bbox_mode': BoxMode.XYXY_ABS,\n",
    "                  'segmentation': [poly],\n",
    "                  'category_id': 0,\n",
    "                  'iscrowd': 0\n",
    "                   }       \n",
    "            objs.append(obj)\n",
    "\n",
    "        record['annotations'] = objs\n",
    "        dataset_dicts.append(record)\n",
    "    return(dataset_dicts)\n",
    "  \n",
    "def get_val_dicts(img_dir):\n",
    "    dataset_dicts = []\n",
    "    for i in range(18,24): \n",
    "        key = 'RGB_frame_' + str(i)       \n",
    "        record = {}       \n",
    "        filename = (val_path + key + '.png')\n",
    "        height, width = ANNO_image.shape[:2]\n",
    "           \n",
    "        record['file_name'] = filename\n",
    "        record['image_id'] = i\n",
    "        record['height'] = height\n",
    "        record['width'] = width\n",
    "\n",
    "        objs = []\n",
    "        for idx, (cell, v) in enumerate(ANNO_val[key].items()):\n",
    "            px = v['x']\n",
    "            py = v['y']           \n",
    "            poly = [(x  , y  )for x, y in zip(px, py)]\n",
    "            poly = [p for x in poly for p in x]\n",
    "            obj = {               \n",
    "                  'bbox': [np.min(px), np.min(py), np.max(px), np.max(py)],\n",
    "                  'bbox_mode': BoxMode.XYXY_ABS,\n",
    "                  'segmentation': [poly],\n",
    "                  'category_id': 0,\n",
    "                  'iscrowd': 0\n",
    "                   }      \n",
    "            objs.append(obj)\n",
    "\n",
    "        record['annotations'] = objs\n",
    "        dataset_dicts.append(record)\n",
    "    return(dataset_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in ['train']:\n",
    "    s = '2'\n",
    "    DatasetCatalog.register( d + '_cell' + s, lambda d=d: get_train_dicts(train_path))\n",
    "    MetadataCatalog.get( d + '_cell' + s).set(thing_classes=['cell'])\n",
    "cell_metadata = MetadataCatalog.get('train_cell' + s)\n",
    "for d in ['val']:\n",
    "    DatasetCatalog.register( d + '_cell' + s, lambda d=d: get_val_dicts(val_path))\n",
    "    MetadataCatalog.get( d + '_cell' + s).set(thing_classes=['cell'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify the data loading is correct, visualize the annotations of randomly selected samples in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this function to visualize data through cv2\n",
    "def cv2_imshow(a, **kwargs):\n",
    "    a = a.clip(0, 255).astype('uint16')\n",
    "    # cv2 stores colors as BGR; convert to RGB\n",
    "    if a.ndim == 3:\n",
    "        if a.shape[2] == 4:\n",
    "            a = cv2.cvtColor(a, cv2.COLOR_BGRA2RGBA)\n",
    "        else:\n",
    "            a = cv2.cvtColor(a, cv2.COLOR_BGR2RGB)\n",
    "    return plt.imshow(a, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dicts = get_train_dicts(train_path)\n",
    "\n",
    "k = random.randint(1,5)\n",
    "for d in range(k):\n",
    "    index = random.randint(0, 17)\n",
    "    img = cv2.imread(train_path+'RGB_frame_'+str(index)+'.png')\n",
    "    visualizer = Visualizer(img, metadata=cell_metadata, scale=0.5)\n",
    "    vis = visualizer.draw_dataset_dict(dataset_dicts[index])\n",
    "    fig = plt.subplots(figsize=(20,20),dpi=300)\n",
    "    plt.axis('off')\n",
    "    cv2_imshow(vis.get_image())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the dataset\n",
    "To avoid overfitting, write our own trainer to train with validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossEvalHook(HookBase):\n",
    "    def __init__(self, eval_period, model, data_loader):\n",
    "        self._model = model\n",
    "        self._period = eval_period\n",
    "        self._data_loader = data_loader\n",
    "    \n",
    "    def _do_loss_eval(self):\n",
    "        # Copying inference_on_dataset from evaluator.py\n",
    "        total = len(self._data_loader)\n",
    "        num_warmup = min(5, total - 1)\n",
    "            \n",
    "        start_time = time.perf_counter()\n",
    "        total_compute_time = 0\n",
    "        losses = []\n",
    "        for idx, inputs in enumerate(self._data_loader):            \n",
    "            if idx == num_warmup:\n",
    "                start_time = time.perf_counter()\n",
    "                total_compute_time = 0\n",
    "            start_compute_time = time.perf_counter()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            total_compute_time += time.perf_counter() - start_compute_time\n",
    "            iters_after_start = idx + 1 - num_warmup * int(idx >= num_warmup)\n",
    "            seconds_per_img = total_compute_time / iters_after_start\n",
    "            if idx >= num_warmup * 2 or seconds_per_img > 5:\n",
    "                total_seconds_per_img = (time.perf_counter() - start_time) / iters_after_start\n",
    "                eta = datetime.timedelta(seconds=int(total_seconds_per_img * (total - idx - 1)))\n",
    "                log_every_n_seconds(\n",
    "                    logging.INFO,\n",
    "                    'Loss on Validation  done {}/{}. {:.4f} s / img. ETA={}'.format(\n",
    "                        idx + 1, total, seconds_per_img, str(eta)),\n",
    "                    n=5,)\n",
    "            loss_batch = self._get_loss(inputs)\n",
    "            losses.append(loss_batch)\n",
    "        mean_loss = np.mean(losses)\n",
    "        self.trainer.storage.put_scalar('validation_loss', mean_loss)\n",
    "        comm.synchronize()\n",
    "        return losses\n",
    "            \n",
    "    def _get_loss(self, data):\n",
    "        # How loss is calculated on train_loop \n",
    "        metrics_dict = self._model(data)\n",
    "        metrics_dict = {\n",
    "            k: v.detach().cpu().item() if isinstance(v, torch.Tensor) else float(v)\n",
    "            for k, v in metrics_dict.items()\n",
    "        }\n",
    "        total_losses_reduced = sum(loss for loss in metrics_dict.values())\n",
    "        return total_losses_reduced\n",
    "        \n",
    "        \n",
    "    def after_step(self):\n",
    "        next_iter = self.trainer.iter + 1\n",
    "        is_final = next_iter == self.trainer.max_iter\n",
    "        if is_final or (self._period > 0 and next_iter % self._period == 0):\n",
    "            self._do_loss_eval()\n",
    "        self.trainer.storage.put_scalars(timetest=12)\n",
    "\n",
    "\n",
    "\n",
    "class MyTrainer(DefaultTrainer):\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "        if output_folder is None:\n",
    "            output_folder = os.path.join(cfg.OUTPUT_DIR, 'inference')\n",
    "        return COCOEvaluator(dataset_name, cfg, True, output_folder)\n",
    "                     \n",
    "    def build_hooks(self):\n",
    "        hooks = super().build_hooks()\n",
    "        hooks.insert(-1,LossEvalHook(\n",
    "            cfg.TEST.EVAL_PERIOD,\n",
    "            self.model,\n",
    "            build_detection_test_loader(\n",
    "                self.cfg,\n",
    "                self.cfg.DATASETS.TEST[0],\n",
    "                DatasetMapper(self.cfg,True))))\n",
    "        return hooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tune a coco-pretrained R50-FPN Mask R-CNN model on our own dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file('COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml'))\n",
    "cfg.DATASETS.TRAIN = ('train_cell'+s,)\n",
    "cfg.DATASETS.TEST = ('val_cell'+s,)\n",
    "cfg.TEST.EVAL_PERIOD = 100  # evaluation period \n",
    "cfg.DATALOADER.NUM_WORKERS = 4\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url('COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml')  # Training initialize from model zoo\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2  #Batch size for ResNet\n",
    "cfg.SOLVER.BASE_LR = 0.00025  # Learning rate\n",
    "cfg.SOLVER.MAX_ITER = 100    # Iterations \n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # Batch size for ROI\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # We only has one class (cell)\n",
    "\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "trainer = MyTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the results\n",
    "Plot training and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_arr(json_path):\n",
    "    lines = []\n",
    "    with open(json_path, 'r') as f:\n",
    "        for line in f:\n",
    "            lines.append(json.loads(line))\n",
    "    return lines\n",
    "\n",
    "experiment_metrics = load_json_arr(cfg.OUTPUT_DIR+'/metrics.json')\n",
    "plt.figure(dpi=200)\n",
    "plt.plot(\n",
    "    [x['iteration'] for x in experiment_metrics], \n",
    "    [x['total_loss'] for x in experiment_metrics])\n",
    "plt.plot(\n",
    "    [x['iteration'] for x in experiment_metrics if 'validation_loss' in x], \n",
    "    [x['validation_loss'] for x in experiment_metrics if 'validation_loss' in x])\n",
    "plt.legend(['total_loss', 'validation_loss'], loc='upper left')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making predictions by loading the model and setting a minimum threshold of 70% certainty at which we’ll consider the predictions as correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set the testing threshold for this model\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a folder and save all images with predicted annotations in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('results', exist_ok=True)\n",
    "for d in range(24,30):\n",
    "    img = cv2.imread(test_path+'RGB_frame_'+str(d)+'.png')\n",
    "    outputs = predictor(img)\n",
    "    v = Visualizer(img[:, :, ::-1],\n",
    "                   metadata=cell_metadata, \n",
    "                   scale=1, \n",
    "                   instance_mode=ColorMode.IMAGE)   # remove the colors of unsegmented pixels\n",
    "    v = v.draw_instance_predictions(outputs['instances'].to('cpu'))\n",
    "    fig = plt.subplots(figsize=(20,20),dpi=300)\n",
    "    plt.axis('off')    \n",
    "    cv2_imshow(v.get_image()[:, :, ::-1])\n",
    "    plt.imsave('./results/test_frame_'+str(d)+'.png',v.get_image()[:, :, ::-1], format = 'png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
